{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://rangle.io\"><img src=\"rangle_wordmark_red.png\" alt=\"Rangle Logo\" style=\"width: 200px;\"/></a>\n",
    "\n",
    "# LDA on Sagemaker\n",
    "\n",
    "This Notebook presents the simple use case of training and deploying a topic model in the cloud.\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "Running this notebook might incure costs. Please make sure you are aware of the costs and don't forget to delete any remaining resources when you're finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who We Are\n",
    "\n",
    "<a href=\"https://rangle.io\">Rangle</a> accelerates digital innovation.\n",
    "We at Rangle believe that Artificial Intelligence (AI) and Machine Learning (ML) have the potential to redefine what it means to build software and provide great user experience.\n",
    "\n",
    "We work with our clients together to integrate AI into their product strategy decisions and build out novel user experiences and smart applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Topic Models?\n",
    "\n",
    "In natural language processing, [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is a generative statistical model that allows categorizing observations (e.g. documents) into groups based on similarity.\n",
    "These groups (or topics) are derived from the data and can be of interest themselves.\n",
    "Topic modeling techniques such as LDA can thus be used to categorized documents and also learn about the topics present in the data.\n",
    "\n",
    "LDA posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics.\n",
    "LDA is an unsupervised learning technique.\n",
    "Therefore the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents.\n",
    "Topics are learned as a probability distribution over the words that occur in each document. \n",
    "Each document, in turn, is described as a mixture of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Amazon Sagemaker?\n",
    "\n",
    "Here we use [Amazon Sagemake](https://aws.amazon.com/sagemaker/) to both train and [deploy](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html) the model.\n",
    "\n",
    "<img src=\"https://docs.aws.amazon.com/sagemaker/latest/dg/images/sagemaker-architecture.png\" alt=\"Rangle Logo\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Sagemaker has the benefit that the user does not have to implement most of the code associated with training and deployment.\n",
    "The whole end-to-end process of training and deploying a model consists of 3 steps.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "Data is transferred to an S3 bucket where it is easily accessible to Model Training.\n",
    "\n",
    "### Model Training\n",
    "\n",
    "Model Training consists of selecting a docker image containing the training code.\n",
    "The user can provide their own custom training code.\n",
    "Below we take advantage of the large library of avaialble machine learning algorithms and just select the 'LDA' image provided by Amazon.\n",
    "The docker image then gets deployed to an [instance](https://aws.amazon.com/sagemaker/pricing/instance-types/) and trains on the data contained in the S3 bucket after which it automatically shuts down to save costs.\n",
    "Selecting the right instance allows the user to trade-off training time and costs.\n",
    "The training concludes by saving the trained model and associated assets to S3.\n",
    "\n",
    "### Deployment\n",
    "\n",
    "Deployment consists of selecting a Deployment instance and the communication protocol.\n",
    "Once the model is deployed the user can use it to make predictions by sending new data to the endpoint, which will answer with the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "The following code will implement the above three steps.\n",
    "\n",
    "- Setup\n",
    "- Data Preparation\n",
    "- Data Visualization\n",
    "- AWS credentials\n",
    "- Training the Model\n",
    "- Evaluating the Model Output\n",
    "- Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import tarfile\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Amazon Web Services (AWS) SDK for Python\n",
    "import boto3\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "# SageMaker Python SDK\n",
    "import sagemaker\n",
    "from sagemaker.amazon.common import numpy_to_record_serializer\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Here we'll use text data collected from the New York Times.\n",
    "The data is already in the right form (word counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word counts\n",
    "with open('nyt_data.txt') as f:\n",
    "    word_counts = f.readlines()\n",
    "# bag_of_words = [x.strip().strip('\\n').strip(\"'\") for x in bag_of_words] \n",
    "word_counts = [x.strip() for x in word_counts] \n",
    "\n",
    "# vocabulary (i.e. unique words)\n",
    "with open('nyt_vocab.dat') as f:\n",
    "    word = f.readlines()\n",
    "word = np.array([x.strip().strip('\\n').strip(\"'\") for x in word])\n",
    "\n",
    "vocabulary_size = len(word)\n",
    "\n",
    "print(len(word_counts), \"bags of words\")\n",
    "print(vocabulary_size, \"words in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranlate bag of words into one giant matrix\n",
    "bags_of_words = np.zeros([len(word_counts), len(word)])\n",
    "\n",
    "for row in range(len(word_counts)):\n",
    "    for col in word_counts[row].split(','):\n",
    "        bags_of_words[row, int(col.split(':')[0])-1] = int(col.split(':')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had some pre-labeled data you'd split the data into training and testing data sets to estimate real-world accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbags_of_words = min(bags_of_words.shape[0], 10_000) # speed up testing with fewer documents\n",
    "ntopics = 10\n",
    "\n",
    "nbags_of_words_training = int(0.95*nbags_of_words)\n",
    "nbags_of_words_test = nbags_of_words - nbags_of_words_training\n",
    "\n",
    "bags_of_words_training = bags_of_words[:nbags_of_words_training]\n",
    "bags_of_words_test = bags_of_words[nbags_of_words_training:nbags_of_words]\n",
    "\n",
    "print('training set dimensions = {}'.format(bags_of_words_training.shape))\n",
    "print('test set dimensions = {}'.format(bags_of_words_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Here we visualize (part of) the matrix we created above.\n",
    "Each row corresponds to a single document (or bag-of-words).\n",
    "Each columns represents a unique word.\n",
    "Columns are roughly sorted according to frequency.\n",
    "The brightness indicates the frequency of a particular word in a particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(bags_of_words[:500, :500], vmax = 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of words in document 3\n",
    "for i in range(10):\n",
    "    print(\"{:10} {}\".format(word[i], int(bags_of_words[3][i])))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lenght of document 3\n",
    "int(bags_of_words[3].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average lenght\n",
    "bags_of_words.sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS credentials\n",
    "\n",
    "All assets should be withing the same region as this notebook instance, training, and hosting.\n",
    "More information can be found at [AWS Identity and Access Management](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns).\n",
    "\n",
    "**Before you proceed:** Modify the `credentials.json` file to include the right role, bucket and prefix, e.g.:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"role\": \"arn:aws:iam::123456789012:role/sagemaker-topic-modeling\",\n",
    "  \"bucket\": \"companyname-topic-modeling-test\",\n",
    "  \"prefix\": \"sagemaker/lda-model\"\n",
    "}\n",
    "```\n",
    "\n",
    "### S3 Bucket\n",
    "\n",
    "Stores training data and model data output.\n",
    "\n",
    "### Prefix\n",
    "\n",
    "The location in the bucket where this notebook's input and and output data will be stored. (The default value is sufficient.)\n",
    "\n",
    "### Role\n",
    "\n",
    "The IAM Role ARN used to give training and hosting access to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_filename = 'credentials.json'\n",
    "\n",
    "with open(credentials_filename) as f:\n",
    "    cred = json.load(f)\n",
    "\n",
    "print('Training input/output will be saved to: {}/{}'.format(cred['bucket'], cred['prefix']))\n",
    "print('IAM Role: {}'.format(cred['role']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Here we fit an LDA model to the corpus (as opposed to generating a corpus).\n",
    "Amazon SageMaker LDA uses a spectral tensor decomposition technique to determine the LDA model parameters $(\\alpha, \\beta)$ which most likely describes the corpus.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png)\n",
    "\n",
    "- $\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions,\n",
    "- $\\beta$ is the parameter of the Dirichlet prior on the per-topic word distribution,\n",
    "- $\\theta_m$ is the topic distribution for document m,\n",
    "- $\\varphi_k$ is the word distribution for topic k,\n",
    "- $z_{mn}$ is the topic for the n-th word in document m, and\n",
    "- $w_{mn}$ is the specific word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to an S3 bucket\n",
    "\n",
    "Here we convert the documents to MXNet RecordIO Protobuf format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 35 s\n",
    "\n",
    "# convert documents_training to Protobuf RecordIO format\n",
    "recordio_protobuf_serializer = numpy_to_record_serializer()\n",
    "fbuffer = recordio_protobuf_serializer(bags_of_words_training)\n",
    "\n",
    "# upload to S3 in bucket/prefix/train\n",
    "fname = 'lda.data'\n",
    "s3_object = os.path.join(cred['prefix'], 'train', fname)\n",
    "boto3.Session().resource('s3').Bucket(cred['bucket']).\\\n",
    "    Object(s3_object).upload_fileobj(fbuffer)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(cred['bucket'], s3_object)\n",
    "print('Uploaded data to S3: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LDA Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker LDA Docker container\n",
    "region_name = boto3.Session().region_name\n",
    "container = get_image_uri(boto3.Session().region_name, 'lda')\n",
    "\n",
    "print('Using SageMaker LDA container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initalize Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "\n",
    "# specify general training job information\n",
    "lda = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    cred['role'],\n",
    "    output_path = 's3://{}/{}/output'.format(cred['bucket'], cred['prefix']),\n",
    "    train_instance_count = 1,\n",
    "    train_instance_type = 'ml.c4.2xlarge',\n",
    "    sagemaker_session = session,\n",
    ")\n",
    "\n",
    "# set algorithm-specific hyperparameters\n",
    "lda.set_hyperparameters(\n",
    "    num_topics = ntopics,\n",
    "    feature_dim = vocabulary_size,\n",
    "    mini_batch_size = nbags_of_words_training,\n",
    "    alpha0 = 1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "After executing the code in the following cell login at:\n",
    "[https://console.aws.amazon.com/console/home](https://console.aws.amazon.com/console/home)\n",
    "\n",
    "And follow the training progress at: `Amazon SageMaker > Training jobs`.\n",
    "(You might have to change the region in the top-right to: `US East`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3:20 min\n",
    "\n",
    "# run the training job on input data stored in S3\n",
    "start = time.time()\n",
    "try:\n",
    "    lda.fit({'train': s3_train_data})\n",
    "except RuntimeError as e:\n",
    "    print(e)  \n",
    "\n",
    "end = time.time()\n",
    "print(\"Training took\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training job name: {}'.format(lda.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model Output\n",
    "\n",
    "Here we download the model assets from Amzon S3 and inspect the model.\n",
    "The model basically constist of two arrays containing the $\\alpha$ and $\\beta$ parameters that were estimated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and extract the model file from S3\n",
    "job_name = lda.latest_training_job.job_name\n",
    "model_fname = 'model.tar.gz'\n",
    "model_object = os.path.join(cred['prefix'], 'output', job_name, 'output', model_fname)\n",
    "boto3.Session().resource('s3').Bucket(cred['bucket']).Object(model_object).download_file(fname)\n",
    "with tarfile.open(fname) as tar:\n",
    "    tar.extractall()\n",
    "print('Downloaded and extracted model tarball: {}'.format(model_object))\n",
    "\n",
    "# obtain the model file\n",
    "model_list = [fname for fname in os.listdir('.') if fname.startswith('model_')]\n",
    "model_fname = model_list[0]\n",
    "print('Found model file: {}'.format(model_fname))\n",
    "\n",
    "# get the model from the model file and store in Numpy arrays\n",
    "alpha, beta = mx.ndarray.load(model_fname)\n",
    "learned_alpha = alpha.asnumpy()\n",
    "learned_beta = beta.asnumpy()\n",
    "\n",
    "print('\\nLearned alpha.shape = {}'.format(learned_alpha.shape))\n",
    "print('Learned beta.shape = {}'.format(learned_beta.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize alpha\n",
    "sns.lineplot(range(len(learned_alpha)), learned_alpha);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize beta\n",
    "sns.heatmap(learned_beta, vmax=0.01); # (topics, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print most important words for a given topic\n",
    "topic_nr = 1\n",
    "\n",
    "beta = learned_beta[topic_nr]\n",
    "idx = np.argsort(beta)\n",
    "\n",
    "print(\"Topic\", topic_nr)\n",
    "print(\"=====================\")\n",
    "for i in idx[:-11:-1]:\n",
    "    print(\"{:12} {:f}\".format(word[i], beta[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "When restricting the number of topics to 10 (`ntopics = 10`), we get the following result for topic 1.\n",
    "\n",
    "```\n",
    "Topic 1\n",
    "=====================\n",
    "music        0.006595\n",
    "play         0.005841\n",
    "life         0.003958\n",
    "performance  0.003903\n",
    "man          0.003707\n",
    "write        0.003601\n",
    "young        0.003494\n",
    "art          0.003459\n",
    "program      0.003393\n",
    "offer        0.003372\n",
    "```\n",
    "\n",
    "Intutitively this seems to correspond to the arts section of a newspaper.\n",
    "Change the `topic_nr` variable above and explore the other 9 topics.\n",
    "What happens if you re-run the training after chaning the `ntopics` variable?\n",
    "Does increasing or decreasing the topic number improve the quality of the topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Here we predict the topic mixture representing a given document.\n",
    "\n",
    "We create an inference endpoint using the SageMaker Python SDK deploy() function from the job we defined above. We specify the instance type where inference is computed as well as an initial number of instances to spin up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 min\n",
    "\n",
    "lda_inference = lda.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Endpoint name: {}'.format(lda_inference.endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure data format (CSV, JSON, RECORDIO Protobu)\n",
    "lda_inference.content_type = 'text/csv'\n",
    "lda_inference.serializer = csv_serializer\n",
    "lda_inference.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query endpoint\n",
    "results = lda_inference.predict(bags_of_words_test[:3])\n",
    "\n",
    "import json\n",
    "print(json.dumps(results, sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's predict on the whole test set\n",
    "results = lda_inference.predict(bags_of_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "sagemaker.Session().delete_endpoint(lda_inference.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
